{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e414afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "// all the libraries which we can use in the eclipse can be imported here \n",
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf28398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadBoringWords: ()Set[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "/* here we load the boring words function which helped us to understand that what we want to load each and every words in a set\n",
    "and specifically we want our data in set only as the items will not b repeated then*/\n",
    "\n",
    "def loadBoringWords():Set[String]={\n",
    "    var boringWords:Set[String]=Set()\n",
    "    val lines=Source.fromFile(\"/home/itv003334/boringwords.txt\").getLines()\n",
    "    for (line<-lines){\n",
    "        boringWords+=line\n",
    "    }\n",
    "    boringWords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a176721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "initial_rddd = user_3 MapPartitionsRDD[3] at textFile at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "user_3 MapPartitionsRDD[3] at textFile at <console>:29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//now we will try to broadcast this data to each node in the cluster which will help us to undestand the results in the cluster.\n",
    "val initial_rddd=sc.textFile(\"user_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "700adf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nameSet = Broadcast(19)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(19)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// we will broadcast the boring words so we can filter out each words again and again and we dont need.\n",
    "var nameSet=sc.broadcast(loadBoringWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5beb785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapped_input = MapPartitionsRDD[7] at map at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at map at <console>:29"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// using the map function as we want to filter our data with respect to our all data\n",
    "val mapped_input=initial_rddd.map(x=>(x.split(\",\")(10).toFloat,x.split(\",\")(0).toLowerCase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a52afed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flat_data = MapPartitionsRDD[8] at flatMapValues at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at flatMapValues at <console>:29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// as we want multiple outputs from single input that too from values we can use the flat Map values function tp get ur output.\n",
    "val flat_data=mapped_input.flatMapValues(x=>x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a79e6e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words = MapPartitionsRDD[10] at map at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at map at <console>:29"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now this flatten data is reduced on the basis of values to get the total final output as the data is in (19.05,big) we have to transpose the data of ourselves\n",
    "val words=flat_data.map(x=>(x._2,x._1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23f84ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_mapped = MapPartitionsRDD[31] at filter at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[31] at filter at <console>:31"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (big,24) is the input we can check whether the data is in  boring list\n",
    "val final_mapped=words.filter(x => !nameSet.value(x._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e3a3bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total = MapPartitionsRDD[37] at sortBy at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[37] at sortBy at <console>:29"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now we can collect the mapped data to form a communication between a link in the data\n",
    "val total=final_mapped.reduceByKey((x,y)=>x+y).sortBy(x=>x._2,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4d70a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((hadoop,4818.34), (intellipaat,2081.22), (analytics,1458.51), (hyderabad,1118.16), (spark,1078.72), (bangalore,1039.27), (cloudxlab,707.52), (bigdata,694.48), (dataflair,643.9), (chennai,604.04), (edureka,351.44), (iit,308.73), (coursera,293.25), (pune,284.71), (curso,277.53), (cloudera,258.06), (simplilearn,252.45001), (scala,250.73), (ameerpet,184.94), (flair,154.13), (acadgild,153.39), (5,147.97), (2020,145.46), (1,122.72), (udemy,115.14), (cours,114.36), (syllabus,112.43), (jigsaw,104.06), (2,102.28999), (msc,98.740005), (gratuito,95.64), (kubernetes,91.93), (kolkata,91.74), (roorkee,83.42), (pilani,81.82), (genomic,78.94), (ourse,78.94), (inceptez,77.3), (3,73.53), (conitive,69.8), (udacity,69.729996), (intelipatt,67.12), (trainin,67.12), (10,6..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// as wetotal.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28120e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
